# FAQ: Watermark Approach dla DES Migration

## ğŸ¤” NajczÄ™Å›ciej Zadawane Pytania

---

### Q1: Czy watermark approach jest bezpieczny dla produkcji?

**A:** TAK, pod warunkiem Å¼e:
- âœ… Archiwizacja jest **sekwencyjna** (starsze pliki â†’ nowsze)
- âœ… MoÅ¼esz zaakceptowaÄ‡ **replay caÅ‚ego okna** w przypadku bÅ‚Ä™du
- âœ… UÅ¼ywasz **external audit logs** zamiast per-file statusu
- âœ… DES packer jest **idempotentny** (te same pliki â†’ te same shardy)

**Uwaga:** Watermark NIE nadaje siÄ™ gdy:
- âŒ Potrzebujesz per-file tracking dla compliance (bez hybrid approach)
- âŒ Archiwizujesz pliki w losowej kolejnoÅ›ci
- âŒ CzÄ™sto wystÄ™pujÄ… czÄ™Å›ciowe bÅ‚Ä™dy wymagajÄ…ce granular retry

---

### Q2: Co siÄ™ stanie jeÅ›li proces padnie w trakcie przetwarzania okna?

**A:** Watermark NIE zostanie zaktualizowany, wiÄ™c nastÄ™pne uruchomienie **przetworzy to samo okno ponownie** (replay).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cycle 1 (FAILED):                               â”‚
â”‚   Window: 2024-11-25 â†’ 2024-11-26              â”‚
â”‚   Processed 800/1000 files                      â”‚
â”‚   âŒ CRASH before watermark update              â”‚
â”‚   Watermark: STILL 2024-11-25                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cycle 2 (RETRY):                                â”‚
â”‚   Window: 2024-11-25 â†’ 2024-11-26 (SAME!)      â”‚
â”‚   Process ALL 1000 files again                  â”‚
â”‚   âœ… Success, watermark updated to 2024-11-26   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dlaczego to jest safe?**
- DES routing jest **deterministyczny**: ten sam (uid, created_at) â†’ ten sam shard
- Drugi run utworzy **identyczne shardy** (overwrites lub idempotent S3 PUTs)
- Zero duplikatÃ³w dziÄ™ki deterministic routing

---

### Q3: Jak Å›ledziÄ‡ ktÃ³re konkretne pliki zostaÅ‚y zarchiwizowane?

**A:** Watermark nie przechowuje per-file status. Masz kilka opcji:

#### Opcja 1: Query based on watermark
```sql
-- SprawdÅº czy plik zostaÅ‚ zarchiwizowany
SELECT 
    CASE 
        WHEN created_at <= (SELECT archived_until FROM des_archive_config WHERE id = 1)
        THEN 'archived'
        ELSE 'pending'
    END as status
FROM files
WHERE uid = 'file-12345';
```

#### Opcja 2: External audit log
```python
# Log kaÅ¼dy spakowany plik do osobnej tabeli
CREATE TABLE des_audit_log (
    uid TEXT,
    created_at TIMESTAMP,
    archived_at TIMESTAMP,
    shard_key TEXT,
    status TEXT
);

# Po spakowaniu:
log_to_audit(uid, created_at, shard_key, status='archived')
```

#### Opcja 3: Hybrid approach (najlepsze z obu Å›wiatÃ³w)
```python
# Migration uÅ¼ywa watermark (fast!)
# Async job aktualizuje archived column (slow but compliance-friendly)

# Nightly job:
UPDATE files 
SET archived = true
WHERE created_at <= (
    SELECT archived_until FROM des_archive_config WHERE id = 1
)
AND archived = false;
```

---

### Q4: Czy mogÄ™ uÅ¼ywaÄ‡ watermark z wieloma workerami (horizontal scaling)?

**A:** TAK! To jest jedna z najwiÄ™kszych zalet watermark approach.

#### Shard-based partitioning:
```yaml
# Worker 1
worker_config:
  shard_id: 0
  shards_total: 10

# Worker 2
worker_config:
  shard_id: 1
  shards_total: 10

# ... Worker 10
worker_config:
  shard_id: 9
  shards_total: 10
```

**Jak to dziaÅ‚a:**
- KaÅ¼dy worker przetwarza **to samo okno czasowe**
- KaÅ¼dy worker filtruje pliki **po shard_id** (hash(uid) % shards_total)
- Zero koordynacji miÄ™dzy workerami
- KaÅ¼dy worker aktualizuje ten sam watermark (idempotent)

```python
# W DatabaseSourceProvider:
async for record in iter_records_for_window(window):
    # Python-level filter by shard
    if hash(record.uid) % shards_total != shard_id:
        continue
    
    yield record  # Tylko pliki dla tego workera
```

**Wynik:**
- 10 workerÃ³w = 10x throughput
- Zero lock contention
- Linearna skalowalnoÅ›Ä‡

---

### Q5: Co jeÅ›li potrzebujÄ™ "cofnÄ…Ä‡" watermark (replay starszych plikÃ³w)?

**A:** MoÅ¼esz manualnie cofnÄ…Ä‡ watermark w des_archive_config:

```sql
-- Cofnij o 1 dzieÅ„ (replay yesterday's files)
UPDATE des_archive_config 
SET archived_until = archived_until - INTERVAL '1 day'
WHERE id = 1;

-- LUB ustaw na konkretnÄ… datÄ™
UPDATE des_archive_config 
SET archived_until = '2024-11-20 00:00:00+00'::timestamp
WHERE id = 1;
```

**UÅ¼yj CLI tool:**
```bash
# Cofnij o 1 dzieÅ„
python3 des_watermark_migrate.py adjust \
  --config config.yaml \
  --days-offset -1

# Ustaw na konkretnÄ… datÄ™
python3 des_watermark_migrate.py adjust \
  --config config.yaml \
  --set-date 2024-11-20
```

**Uwaga:** Replay utworzy identyczne shardy (dziÄ™ki deterministic routing), wiÄ™c jest safe.

---

### Q6: Jak monitorowaÄ‡ postÄ™p migracji?

**A:** UÅ¼yj Prometheus metrics + Grafana dashboard:

#### Kluczowe metryki:
```promql
# Lag miÄ™dzy watermark a target
des_watermark_lag_seconds

# Pliki per window
rate(des_migration_files_total[5m])

# Czas przetwarzania window
des_migration_duration_seconds

# Rozmiar window
des_watermark_window_size_seconds
```

#### SQL queries:
```sql
-- Aktualny lag
SELECT 
    archived_until,
    NOW() - INTERVAL '7 days' AS target_cutoff,
    (NOW() - INTERVAL '7 days') - archived_until AS lag
FROM des_archive_config
WHERE id = 1;

-- Pending files w current window
SELECT COUNT(*) as pending_files
FROM files
WHERE created_at > (SELECT archived_until FROM des_archive_config WHERE id = 1)
  AND created_at <= NOW() - INTERVAL '7 days';
```

#### CLI tool:
```bash
# PokaÅ¼ statystyki
python3 des_watermark_migrate.py stats --config config.yaml

# Output:
# Current watermark:    2024-11-25
# Target cutoff:        2024-11-26
# Lag behind target:    1 days
# Files pending:        45,678
# Total size:           89.3 GB
```

---

### Q7: Co siÄ™ stanie z kolumnÄ… `archived` po migracji?

**A:** Masz 3 opcje:

#### Opcja 1: UsuÅ„ kolumnÄ™ (najbardziej radykalna)
```sql
-- UWAGA: Destructive! ZrÃ³b backup!
ALTER TABLE files DROP COLUMN archived;
DROP INDEX idx_files_created_archived;

-- KorzyÅ›Ä‡: Mniejsza tabela, prostszy schema
-- Wada: Nie moÅ¼na wrÃ³ciÄ‡ do per-record bez migracji
```

#### Opcja 2: Zostaw ale ignoruj (bezpieczna)
```sql
-- Kolumna archived pozostaje ale nie jest uÅ¼ywana
-- Query uÅ¼ywa tylko created_at vs watermark
-- Kolumna moÅ¼e zawieraÄ‡ stare wartoÅ›ci (nie szkodzi)

-- KorzyÅ›Ä‡: Åatwy rollback do per-record
-- Wada: NieuÅ¼ywana kolumna zajmuje miejsce
```

#### Opcja 3: Sync async (hybrid approach)
```sql
-- Migration uÅ¼ywa watermark (fast)
-- Nightly job aktualizuje archived (compliance)

-- Cron: 2am daily
UPDATE files 
SET archived = true
WHERE created_at <= (SELECT archived_until FROM des_archive_config WHERE id = 1)
  AND archived = false
LIMIT 1000000;  -- Rate limit

-- KorzyÅ›Ä‡: Compliance + performance
-- Wada: Kolumna archived moÅ¼e byÄ‡ "za" watermarkiem (max 24h)
```

**Rekomendacja:** Opcja 3 (hybrid) dla wiÄ™kszoÅ›ci przypadkÃ³w.

---

### Q8: Czy watermark approach jest zgodny z compliance (SEC 17a-4, HIPAA)?

**A:** To zaleÅ¼y od wymagaÅ„:

#### âœ… Watermark moÅ¼e byÄ‡ compliance-ready gdy:
- Prowadzisz **external audit logs** dla kaÅ¼dego pliku
- Audit log zawiera: uid, timestamp, shard_key, operation
- MoÅ¼esz udowodniÄ‡ Å¼e plik zostaÅ‚ zarchiwizowany (przez DES retriever)
- UÅ¼ywasz **hybrid approach** z async update `archived` column

#### âŒ Watermark NIE speÅ‚nia compliance gdy:
- Wymaga siÄ™ **atomowego** per-file statusu w source DB
- Audit trail musi byÄ‡ **wewnÄ…trz transakcji** z gÅ‚Ã³wnÄ… tabelÄ…
- Wymagane jest **instant** per-file tracking (bez lag)

**Hybrid approach dla compliance:**
```python
# 1. Migration uÅ¼ywa watermark (fast, zero overhead)
# 2. Audit log zapisuje kaÅ¼dy spakowany plik
# 3. Nightly job aktualizuje archived column (compliance)

# Rezultat:
# - Fast migration (watermark)
# - Compliance (archived column + audit log)
# - Max lag: 24h (akceptowalne dla wiÄ™kszoÅ›ci regulacji)
```

---

### Q9: Jak testowaÄ‡ watermark migration przed production deploy?

**A:** UÅ¼yj 3-stage approach:

#### Stage 1: Dry-run w dev
```bash
# Test z demo database
python3 demo_comparison.py --records 10000

# Output pokazuje rÃ³Å¼nicÄ™ w performance
```

#### Stage 2: Shadow mode w staging
```python
# Uruchom obie Å›cieÅ¼ki rÃ³wnolegle, porÃ³wnaj wyniki
class ShadowMigrationOrchestrator:
    def run_cycle(self):
        # A. Per-record (current)
        result_old = per_record_orchestrator.run_cycle()
        
        # B. Watermark (new)
        result_new = watermark_orchestrator.run_cycle()
        
        # C. Verify: same files processed
        assert result_old.files_processed == result_new.files_processed
        
        # D. Verify: same shards created (deterministic!)
        assert compare_shards(result_old.shards, result_new.shards)
```

#### Stage 3: Canary deployment w prod
```yaml
# Day 1: 10% traffic to watermark
worker_per_record: 9 replicas
worker_watermark: 1 replica

# Day 3: 50% traffic
worker_per_record: 5 replicas
worker_watermark: 5 replicas

# Day 7: 100% watermark
worker_per_record: 0 replicas
worker_watermark: 10 replicas
```

---

### Q10: Co jeÅ›li niektÃ³re pliki w oknie siÄ™ nie powiodÄ… (validation errors)?

**A:** To jest najwiÄ™ksza rÃ³Å¼nica miÄ™dzy per-record a watermark:

#### Per-Record approach:
```
Files in batch: [A, B, C(fail), D, E]
                 â†“
Pack successful: [A, B, D, E]
                 â†“
UPDATE archived=true: [A, B, D, E]  â† Tylko successful!
                 â†“
Next batch will see: [C, F, G, ...]  â† C is retried
```

#### Watermark approach:
```
Files in window: [A, B, C(fail), D, E]
                 â†“
Pack attempt: [A, B, C(FAIL), D, E]
                 â†“
Options:
1. Stop on error: watermark NOT advanced, entire window replayed
2. Continue on error: watermark advanced, C is SKIPPED!
```

**RozwiÄ…zanie: Error isolation + retry logic**
```python
# Opcja 1: Skip failures, log to error table
CREATE TABLE des_migration_errors (
    uid TEXT,
    created_at TIMESTAMP,
    error_message TEXT,
    retry_count INTEGER,
    failed_at TIMESTAMP
);

# Po migracji: manual retry errors
SELECT * FROM des_migration_errors
WHERE retry_count < 3;

# Opcja 2: Separate error window
# Watermark advances past successful files
# Error files get separate processing (out-of-band)
```

**Best practice:**
- Use error isolation (don't block entire window)
- Log failures to separate table
- Manual review/retry failed files
- Alert on high failure rate

---

### Q11: Czy mogÄ™ uÅ¼ywaÄ‡ watermark z S3 source files?

**A:** TAK! Watermark approach dziaÅ‚a identycznie z S3:

```python
from des_core.file_reader import S3FileReader

# Configure S3 reader
file_reader = S3FileReader(
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY"),
    aws_secret_access_key=os.getenv("AWS_SECRET_KEY"),
    region_name="us-east-1"
)

# Create orchestrator with S3 reader
orchestrator = WatermarkMigrationOrchestrator(
    db_connection=conn,
    config_connection=config_conn,
    source_config=source_config,
    packer_config=packer_config,
    file_reader=file_reader,  # â† S3FileReader!
)

# Database schema (same as local):
CREATE TABLE files (
    uid TEXT PRIMARY KEY,
    created_at TIMESTAMP,
    file_location TEXT,  -- s3://bucket/key/file.dat
    size_bytes BIGINT
);
```

**Performance tips:**
- Use S3 batch operations gdy moÅ¼liwe
- Prefetch file metadata (HEAD requests)
- Use multipart download dla duÅ¼ych plikÃ³w

---

### Q12: Jak dÅ‚ugo powinien byÄ‡ `lag_days`?

**A:** To zaleÅ¼y od przypadku uÅ¼ycia:

| Use Case | Recommended lag_days | Reasoning |
|----------|---------------------|-----------|
| **Real-time archive** | 1-2 days | Minimal lag, fast archiving |
| **Standard archive** | 7 days | Buffer for errors, safe default |
| **Compliance archive** | 30 days | Extra safety, audit window |
| **Cold storage** | 90+ days | Only very old files |

**Trade-offs:**
- **MaÅ‚y lag (1-2 dni):**
  - âœ… Szybsze archiwizowanie
  - âœ… Mniej pending files
  - âŒ Mniej czasu na wykrycie bÅ‚Ä™dÃ³w
  
- **DuÅ¼y lag (30+ dni):**
  - âœ… WiÄ™cej czasu na weryfikacjÄ™
  - âœ… Buffor dla bÅ‚Ä™dÃ³w/zmian
  - âŒ WiÄ™cej pending files w queue

**Best practice:** Start with 7 days, adjust based on monitoring.

---

### Q13: Co z transaction safety przy watermark approach?

**A:** Watermark uÅ¼ywa **eventual consistency** zamiast strict transactions:

#### Per-Record (strict transactions):
```python
with db.begin():
    # Pack files
    pack_files(files)
    # Mark as archived (same transaction)
    UPDATE files SET archived=true WHERE uid IN (...)
# Either both succeed or both rollback atomically
```

#### Watermark (eventual consistency):
```python
# Phase 1: Pack files (idempotent, no transaction)
try:
    pack_files(files)
except:
    # Failure: watermark NOT advanced
    # Next run will replay window (safe!)
    return

# Phase 2: Advance watermark (separate, atomic)
UPDATE des_archive_config SET archived_until = ...
```

**Safety guarantees:**
- âœ… **Idempotency:** Replay window = same shards
- âœ… **No data loss:** Failed pack = watermark not advanced
- âœ… **No duplicates:** Deterministic routing prevents doubles
- âŒ **Not atomic:** Pack can succeed but watermark update fail (rare)

**Handling edge case (watermark update fails):**
```python
try:
    # Pack files
    shards = pack_files(...)
    
    # Try to advance watermark
    try:
        advance_watermark(target_cutoff)
    except:
        # Watermark update failed!
        # Log error, alert operator
        logger.error("Watermark update failed but files were packed!")
        # Next run will replay window
        # DES routing ensures same shards = safe
```

---

### Q14: Jaki jest sizing dla des_archive_config table?

**A:** BARDZO maÅ‚a - zawsze 1 wiersz!

```sql
-- Table size
SELECT pg_size_pretty(pg_total_relation_size('des_archive_config'));
-- Output: 8 kB (includes indexes)

-- Row count
SELECT COUNT(*) FROM des_archive_config;
-- Output: 1 (always!)

-- UPDATE performance
EXPLAIN ANALYZE 
UPDATE des_archive_config SET archived_until = NOW() WHERE id = 1;
-- Execution time: 0.123 ms (sub-millisecond!)
```

**Dlaczego to jest tak szybkie?**
- 1 wiersz = instant lookup (no scan)
- Brak indeksu (nie potrzebny dla 1 row)
- UPDATE in-place (no reordering)
- Zero lock contention (single row)

**PorÃ³wnanie:**
```
Per-Record:
  UPDATE 1,000,000 rows = ~5-10 minutes (depends on hardware)
  Transaction log: 50+ GB
  Lock time: seconds

Watermark:
  UPDATE 1 row = ~0.1 milliseconds
  Transaction log: 100 bytes
  Lock time: microseconds
```

---

### Q15: Czy mogÄ™ migrowaÄ‡ tylko czÄ™Å›ciowo (niektÃ³re pliki per-record, niektÃ³re watermark)?

**A:** NIE, nie jest to zalecane. Ale moÅ¼esz mieÄ‡ **2 osobne tabele**:

#### Opcja 1: Split tables (zalecane)
```sql
-- Tabela 1: Hot data (per-record)
CREATE TABLE files_hot (
    uid TEXT PRIMARY KEY,
    created_at TIMESTAMP,
    file_location TEXT,
    archived BOOLEAN DEFAULT FALSE  -- per-record tracking
);

-- Tabela 2: Cold data (watermark)
CREATE TABLE files_cold (
    uid TEXT PRIMARY KEY,
    created_at TIMESTAMP,
    file_location TEXT
    -- NO archived column!
);

-- Separate des_archive_config for cold table
CREATE TABLE des_archive_config_cold (
    id INTEGER PRIMARY KEY,
    archived_until TIMESTAMP,
    lag_days INTEGER
);
```

**Migration flow:**
```python
# 1. Hot files use per-record (small volume, need tracking)
hot_orchestrator = MigrationOrchestrator(table="files_hot")

# 2. Cold files use watermark (huge volume, need speed)
cold_orchestrator = WatermarkMigrationOrchestrator(table="files_cold")

# Run both:
hot_result = hot_orchestrator.run_cycle()
cold_result = await cold_orchestrator.run_cycle()
```

#### Opcja 2: Time-based split (nie zalecane)
```python
# Files < 30 days: per-record
# Files > 30 days: watermark

# Problem: Mixing approaches in same table is confusing!
# Better to use Option 1 (split tables)
```

---

## ğŸ¯ Podsumowanie FAQ

**NajwaÅ¼niejsze punkty:**

1. âœ… **Watermark jest production-ready** dla sequential archiving
2. âœ… **Replay window jest safe** dziÄ™ki idempotency
3. âœ… **Horizontal scaling dziaÅ‚a** out-of-the-box
4. âœ… **Compliance moÅ¼liwy** przez hybrid approach
5. âš ï¸ **Trade-off:** Window-based tracking vs per-file precision

**Kiedy uÅ¼ywaÄ‡ watermark:**
- Masz >100M plikÃ³w
- UPDATE overhead jest problemem  
- MoÅ¼esz zaakceptowaÄ‡ window-based tracking
- Priorytet: maksymalna wydajnoÅ›Ä‡

**Kiedy NIE uÅ¼ywaÄ‡ watermark:**
- Potrzebujesz atomic per-file tracking
- Archiwizacja nie jest sekwencyjna
- <10M plikÃ³w (overhead nie jest problemem)
- Strict compliance wymaga per-file status

**Hybrid approach = Best of both worlds!**
- Fast migration (watermark)
- Compliance tracking (archived column updated async)
- External audit logs (complete trail)
